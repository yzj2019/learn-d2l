{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、经典机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。\n",
    "- 此外，当时的数据集仍然相对较小。\n",
    "- 除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1、经典机器学习流水线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）；\n",
    "- 根据光学、几何学、其他知识以及偶然的发现，手工对数据集进行预处理；\n",
    "- 通过标准的特征提取算法，如SIFT（尺度不变特征变换，2004）和SURF（加速鲁棒特征，2006）或其他手动调整的流水线来输入数据；\n",
    "- 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "凸优化问题，漂亮的定理，优雅的理论去证明模型的各种性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2、深度学习之前的CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更关注特征提取，不太关注后续的分类用机器学习模型；\n",
    "- 关注几何特征的描述（如多相机）；\n",
    "- 通过前提假设的限制，试图建模成凸优化问题，相应的有漂亮定理证明；\n",
    "- 如果前提假设满足了，效果会更好。\n",
    "\n",
    "SIFT（尺度不变特征变换，2004）、SURF（加速鲁棒特征，2006）、HOG（定向梯度直方图，2005）、bags of visual words（聚类）；\n",
    "\n",
    "另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。\n",
    "\n",
    "有趣的是，在AlexNet网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3、深度网络的突破"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figs/1.jpg' style=\"zoom:20%;\" />\n",
    "\n",
    "90年前后，神经网络，模型小；00年前后，核方法，简单且理论完备；数据增长的速度比算力增长的速度慢，使得有能力进一步挖掘数据中的东西。\n",
    "\n",
    "深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。\n",
    "- 数据：\n",
    "   - 包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。\n",
    "   - 2010，大数据浪潮；2009，ImageNet数据集（李飞飞等）；\n",
    "- 硬件：\n",
    "   - 深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据；\n",
    "   - 这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选；\n",
    "   - 用GPU做神经网络训练：高带宽，单GPU卡多芯片、单片多组、单组多处理单元的算力堆叠，并行化处理矩阵运算（cuda）；\n",
    "- ImageNet数据集：\n",
    "   - 自然物体的彩色图片；\n",
    "   - $469 \\times 387$；\n",
    "   - 本身1千万，用来做竞赛的1.2M张；\n",
    "   - 1000类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet赢得了2012年ImageNet竞赛；\n",
    "- 更深更大的LeNet；\n",
    "- 主要改进：\n",
    "  - 丢弃法：模型复杂度的控制，训练时加入无偏噪音，保证模型泛化性；\n",
    "  - ReLU：梯度比Sigmoid更大，且在0点处的一阶导数更好一点，支持更深的模型；\n",
    "  - MaxPooling：取最大值，输出值相对较大，梯度较大，使得训练更加容易；\n",
    "- 引起了计算机视觉方法论的改变！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1、模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://zh-v2.d2l.ai/_images/alexnet.svg)\n",
    "\n",
    "更深；更大的池化窗口，更大的核窗口和步长，因为图片更大了；更多输出通道；\n",
    "- 激活函数从Sigmoid变成了ReLU，减缓梯度消失；\n",
    "- 隐藏全连接层后加入了Dropout；\n",
    "- 数据增强：随机翻转、随机截取图片的部分，随机调整亮度，随机调整色温；希望训练出的特征，对其他因素不敏感。\n",
    "\n",
    "复杂度对比：\n",
    "\n",
    "<img src='./figs/2.jpg' style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet是更大更深的LeNet，$10\\times$参数个数，$260\\times$计算复杂度；\n",
    "- 新引入丢弃法、ReLU、最大池化层、数据增强，使得训练更容易；\n",
    "- AlexNet赢下2012ImageNet竞赛后，标志着新一轮神经网络热潮开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。这里没有采用这一点。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88474ca0ac5bd289cfc2b7cff5e070fc2eb64be0b001ce0011ec337ea9a55b21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
