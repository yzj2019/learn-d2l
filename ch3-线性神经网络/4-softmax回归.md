## softmax回归

前面学习了回归问题，下面是分类问题。softmax回归实际上是分类问题。

### 1.从回归到多分类

- 回归估计一个连续值，而分类预测一个离散类别；
- 回归：
   - 单连续数值输出；
   - 自然区间R；
   - 跟真实值的区别作为损失。
- 分类：
   - 通常多个输出；
   - **类别之间没有自然顺序，或问题与类别的自然顺序无关；**
   - 输出i是预测为第i类的置信度。
- 如何从回归问题到多分类问题？
   - 表示：one-hot编码来表示类别，使得能与网络输出的置信度“可比较”；
   - 前向计算：希望训练网络，使得能输出预测的置信度，同时网络输出的预测值中，真实类别的置信度尽可能接近于1，而其它类别的置信度尽可能接近于0；为此引入softmax计算；
   - 损失：引入交叉熵损失，来衡量两个概率分布之间的差距。

### 2.softmax运算和交叉熵损失

- $\hat{\mathbf{y}} = softmax(\mathbf{o})$，其中$\mathbf{o}$由线性层得到，而$\hat{y}_j = \frac{exp(o_j)}{\sum_kexp(o_k)}$

- 交叉熵常用来衡量两个概率分布之间的区别：$H(p,q)=\sum_i-p_ilog(q_i)$

   - 作为损失：$l(\mathbf{y},\hat{\mathbf{y}}) = -\sum_iy_ilog\hat{y_i} = -log(\hat{y}_y)$，即使得真实类别的预测置信度尽可能接近1；

   - 梯度：q为类别总数，n为样本总数
     $$
     \begin{aligned}
     l(\mathbf{y}, \hat{\mathbf{y}}) &= -\sum_{j=1}^q
     y_jlog\frac{exp(o_j)}{\sum_{k=1}^qexp(o_k)} \\
     &= \sum_{j=1}^qy_jlog\sum_{k=1}^qexp(o_k) - \sum_{j=1}^qy_jo_j \\
     &= log\sum_{k=1}^qexp(o_k) - \sum_{j=1}^qy_jo_j
     \end{aligned}
     $$
     对输入$o_j$的偏导数（梯度）：
     $$
     \partial_{o_j}l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{exp(o_j)}{\sum_{k=1}^qexp(o_k)} - y_j = softmax(\mathbf{o})_j - y_j
     $$
     即为观测值$\mathbf{y}$和估计值$\hat{\mathbf{y}}$之间的差距。

- cross entropy与信息论：

   - 压缩与预测的关系：当数据易于预测，也就易于压缩。

   - 用概率的负对数量化“信息量”；分布的熵：预期信息量。

   - 可以把从P到Q的交叉熵H(P,Q)，想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期信息量增益”。
   - 总之，可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。

### 3.小结

- softmax运算获取一个向量并将其映射为概率。
- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。