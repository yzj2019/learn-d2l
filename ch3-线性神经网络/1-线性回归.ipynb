{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们想预测一个数值时，就会涉及到回归问题。但不是所有的预测都是回归问题。分类问题的目标是预测数据属于一组类别中的哪一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.线性回归的基本元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归基于几个简单的假设：\n",
    "  - 假设自变量$x$和因变量$y$之间的关系是线性的， 即$y$可以表示为$x$中元素的加权和，这里通常允许包含观测值的一些噪声；\n",
    "  - 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 给定n维输入$\\mathbf{x}=[x_1,x_2,...,x_n]^T$；线性模型包含：\n",
    "    - n维权重(weight)：$\\mathbf{w} = [w_1,w_2,...,w_n]^T$\n",
    "    - 标量偏置(bias)/偏移量(offset)/截距(intercept)：$b$\n",
    "  - 输出是输入的加权和：$\\hat{y} = \\mathbf{w}^T\\mathbf{x} + b$，是输入特征的一个仿射变换（affine transformation）。仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。\n",
    "  - 可以看作单层神经网络。\n",
    "用符号表示的矩阵$\\mathbf{X} \\in \\mathbb{R}^{n*d}$可以很方便地引用我们整个数据集的n个样本。其中，$\\mathbf{X}$的每一行是一个样本，每一列是一种特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2损失函数\n",
    "  - 一种模型质量的度量方式。损失函数（loss function）能够量化目标的实际值与预测值之间的差距。\n",
    "  - 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差：在训练集n个样本上的平方误差均值$L(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}, b) = \\frac{1}{2n}||\\mathbf{y} - \\hat{\\mathbf{y}}||^2 = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X}\\mathbf{w} - b||^2$；\n",
    "\n",
    "在训练模型时，我们希望寻找一组参数$(\\mathbf{w}^*, b^*)$， 这组参数能最小化在所有训练样本上的总损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3解析解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$\\frac{\\partial}{\\partial{\\mathbf{w}}} L(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}, b) = 0$，得到$\\mathbf{w}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.基础优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 挑选初始值$\\mathbf{w_0}$\n",
    "  - 重复迭代参数t=1,2,3；$\\mathbf{w}_t = \\mathbf{w}_{t-1} - \\eta \\frac{\\partial{L}}{\\partial{\\mathbf{w}_{t-1}}}$\n",
    "    - 沿梯度反方向，将减小损失函数值\n",
    "    - $\\eta$学习率：步长的超参数\n",
    "  - 选择学习率：\n",
    "    - 不能太小，因为计算梯度很贵，不能算太多；\n",
    "    - 也不能太大，会震荡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2小批量随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 在整个训练集上算梯度太贵；\n",
    "  - 可以随机采样b个样本$i_1,i_2,...,i_b$来近似代替整个训练集上的梯度：$\\frac{1}{b}\\sum_{i\\in I_b}{L(\\mathbf{X}_i, y_i, \\mathbf{W})}$；\n",
    "  - b是批量大小，另一个重要的超参数：\n",
    "    - 不能太小，不具有代表性；\n",
    "    - 也不能太大，浪费计算，失去了本身的意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.矢量化加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "希望对整个mini batch进行处理，将计算转为矩阵运算，避免在python中编写高代价的循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "n = 10000\n",
    "a = torch.ones(n)\n",
    "b = torch.ones(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10408 sec'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros(n)\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00075 sec'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重载运算符的矩阵运算具有明显的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.正态分布与均方损失\n",
    "通过对噪声分布的假设来解读均方损失目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从均值为0的正态分布。\n",
    "$$\n",
    "y = \\mathbf{w}^T\\mathbf{x} + b + \\epsilon, ~ \\epsilon \\sim \\mathcal{N}(0, \\sigma)\n",
    "$$\n",
    "通过给定的$\\mathbf{x}$观测到特定$y$的似然（likelihood）：$P(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left(-\\frac{1}{2\\sigma^2}(y - \\mathbf{w}^T\\mathbf{x} - b)^2\\right)$\n",
    "\n",
    "极大似然，最小化负对数似然：$-logP(\\mathbf{y}|\\mathbf{X}) = \\sum_{i=1}^n \\frac{1}{2}log(2\\pi \\sigma^2) + \\frac{1}{2\\sigma^2}(y^{(i)} - \\mathbf{w}^T\\mathbf{x}^{(i)} - b)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.从线性回归到深度网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。\n",
    "  - 矢量化使数学表达上更简洁，同时运行的更快。\n",
    "  - 最小化目标函数和执行极大似然估计等价。\n",
    "  - 线性回归模型也是一个简单的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2}exp(-|\\epsilon|)$\n",
    "  - 写出模型$−logP(\\mathbf{y}∣\\mathbf{X})$下数据的负对数似然：$\\sum_{i=1}^n log(2) + |y^{(i)} - \\mathbf{w}^T\\mathbf{x}^{(i)} - b|$\n",
    "  - 你能写出解析解吗？不能\n",
    "  - 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）你能解决这个问题吗？\n",
    "    - 对比上面的高斯噪声采用的mean square error(MSE)，此处使用mean absolute error(MAE)作为损失函数。\n",
    "    - MAE在零点附近不可导，故在驻点附近仍然可能产生很大的梯度，导致剧烈波动难以收敛。\n",
    "    - 一般可以采用smooth L1-loss代替L1-loss，取得更稳定的收敛结果。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88474ca0ac5bd289cfc2b7cff5e070fc2eb64be0b001ce0011ec337ea9a55b21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
