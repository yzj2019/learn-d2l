{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值稳定性、模型初始化和激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.梯度消失和梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们需要进行梯度的反向传播，需要连乘；对比较深的神经网络，当梯度值过大或过小时，连乘的结果就会上溢或下溢。\n",
    "- 梯度爆炸的问题：\n",
    "  - 值超出值域（infinity）\n",
    "    - 对于16位浮点数尤为严重(6e-5, 6e4)\n",
    "  - 对学习率敏感\n",
    "    - 学习率太大=》大参数值=》更大的梯度\n",
    "    - 学习率太小=》训练没有进展\n",
    "    - 我们可能需要在训练过程中，不断调整学习率\n",
    "- 梯度消失的问题：\n",
    "  - 梯度值变成0，特别是16位浮点数\n",
    "  - 训练没有进展，不管如何选择学习率\n",
    "  - 对底层尤为严重\n",
    "    - 仅仅顶部层训练的较好，底部由于梯度值太小，没啥效果\n",
    "    - 无法让神经网络更深\n",
    "- 参数化自带的对称性问题：不可初始化权重W为常数值！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.让训练更加稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标：让梯度值在合理的范围内，如[1e-6, 1e3]\n",
    "- 将部分乘法变加法：ResNet、LSTM\n",
    "- 归一化：梯度归一化、梯度剪裁\n",
    "- 合理的权重初始化和激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决（或至少减轻）上述问题的一种方法是进行参数初始化；优化期间的注意和适当的正则化也可以进一步提高稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在训练开始的时候更容易有数值不稳定\n",
    "  - 远离最优解的地方，损失函数表面可能很复杂\n",
    "- 前面我们常用正态分布来做初始化，对小网络没问题，但不能保证深度神经网络也。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Xavier初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 思想来源：\n",
    "  - 考察单个线性层，输入和输出维度分别为$n_{in},n_{out}$，输入为x，输出表示为：$o_i = \\sum_{j=1}{n_{in}}w_{ij}x_j$\n",
    "    - 假设权重$w_{ij}$独立同分布，均值为0方差为$\\sigma^2$；输入值$x_j$独立同分布，均值为0方差为$\\gamma^2$，且与权重$w_{ij}$互相独立\n",
    "    - 计算输出$o_i$的均值和方差，可得：$E[o_i]=0, ~ Var[o_i] = n_{in}\\sigma^2\\gamma^2$\n",
    "  - 想要保持本层的输入方差和输出方差一致，则需要设置$n_{in}\\sigma^2=1$；\n",
    "  - 相应地，考察上面模型的梯度反向传播，想保持梯度的方差前后一致，则需要设置$n_{out}\\sigma^2=1$；\n",
    "  - 折中，取$\\frac{1}{2}(n_{in}+n_{out})\\sigma^2=0$\n",
    "- 实践中：虽然不总是线性的，但实践证明有效\n",
    "  - 一般从高斯分布中采样，$\\mathcal{N}(0, \\frac{2}{n_{in}+n_{out}})$\n",
    "  - 也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差：$\\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}})$\n",
    "    - 分布$\\mathcal{U}[-a,a]$的方差是$\\frac{a^2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2拓展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习框架通常实现十几种不同的启发式方法。此外，参数初始化一直是深度学习基础研究的热点领域。\n",
    "\n",
    "其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采用类似的思想观察激活函数，目标是使得输入和输出的均值、方差保持一致。\n",
    "- 考察线性函数的均值和方差，正向传播和反向传播：\n",
    "  - 对$y = ax+b$，若要保持一致，需要$b=0 \\& a=1$\n",
    "- 使用泰勒展开，观察常用激活函数：\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "    sigmoid(x) &= \\frac{1}{2} + \\frac{x}{4} - \\frac{x^3}{48} + O(x^5) \\\\\n",
    "    tanh(x) &= 0 + x - \\frac{x^3}{3} +O(x^5) \\\\\n",
    "    relu(x) &= 0 + x\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "- 调整sigmoid：$4 * sigmoid(x) -2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。\n",
    "- 需要用启发式的初始化方法来确保初始梯度既不太大也不太小。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88474ca0ac5bd289cfc2b7cff5e070fc2eb64be0b001ce0011ec337ea9a55b21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
