## 多层感知机

### 1.感知机

#### 1.1定义

给定输入$X$，权重$W$和偏移$b$，感知机输出：
$$
o = \sigma(<W,X>+b), ~ \sigma(x)=\begin{cases}
1 & if ~ x>0 \\
-1 & otherwise
\end{cases}
$$
二分类：-1或1

- vs 回归输出实数
- vs softmax回归输出概率

#### 1.2训练

- 初始化W=0、b=0；
- 如果分类错误，即$y_i*[<W, x_i>+b] \leq 0$，则$W \leftarrow W+y_ix_i$，$b\leftarrow b+y_i$；
- 等价于使用批量大小为1的梯度下降，$Loss(y,x,W)=max(0,-y<W,x>)$。

#### 1.3收敛定理

- 若：
  - 数据在半径r的区域内；
  - 存在分界面，使得分类都正确且余量为$\rho$：$y(X^TW+b)\geq \rho$对于$||W||^2+b^2 \leq 1$；
- 则感知机保证在$\frac{r^2+1}{\rho^2}$步后收敛。

#### 1.4问题

不能拟合XOR函数，只能产生线性分界面（1969，第一次寒冬）。

#### 1.5小结

- 感知机是二分类模型，最早的AI模型之一；
- 求解算法等价于批量大小为1的梯度下降；
- 不能拟合XOR函数，导致第一次AI寒冬。

### 2.多层感知机

#### 2.1在网络中加入隐藏层

从隐藏层的激活函数中，加入非线性；

隐藏层大小为超参数；

如单隐藏层：

- 输入：$X\in \mathbb{R}^n$；
- 隐藏层：$W_1 \in \mathbb{R}^{m*n}，b_1\in \mathbb{R}$，$\sigma$为按元素的激活函数；$h = \sigma(W_1X+b_1)$；
- 输出层：$W_2 \in \mathbb{R}^{1*m}，b_2\in \mathbb{R}$；$o = W_2h+b_2$

#### 2.2通用近似定理

即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。