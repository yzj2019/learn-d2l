{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo-v1的pytorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依照yolov1.cfg、yolov1原论文、[github上一份pytorch实现](https://github.com/ProgrammerZhujinming/YOLOv1/blob/main/YOLO_V1_Model.py)；\n",
    "\n",
    "yolov1.cfg最后的fc层内容没太看懂；\n",
    "\n",
    "module.py是方便fine-tune的版本；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试填充函数：\n",
    "- 输入的tensor：（batch，channel，height， width）\n",
    "- 填充pad：[宽左，宽右，高上，高下]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4, 4]),\n",
       " tensor([[[[ 1.1267, -0.7697, -0.4392, -0.2969],\n",
       "           [ 0.2807,  1.3941, -0.6943,  0.6952],\n",
       "           [-0.0919,  0.9841,  0.7246,  1.4224],\n",
       "           [-0.4062, -1.9781, -1.8463,  1.3334]]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 1, 4, 4))\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11, 7]),\n",
       " tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  1.1267, -0.7697, -0.4392, -0.2969,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.2807,  1.3941, -0.6943,  0.6952,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.0919,  0.9841,  0.7246,  1.4224,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.4062, -1.9781, -1.8463,  1.3334,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = F.pad(x, pad=[1,2,3,4])\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11, 7]),\n",
       " tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  1.1267, -0.7697, -0.4392, -0.2969,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.2807,  1.3941, -0.6943,  0.6952,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.0919,  0.9841,  0.7246,  1.4224,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.4062, -1.9781, -1.8463,  1.3334,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = nn.ZeroPad2d([1,2,3,4])\n",
    "y = f(x)\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = y[0, 0, 0, :4]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这个自适应padding形状的函数没用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_padding_shape(input_shape, kernel_size, stride):\n",
    "    '''\n",
    "    根据输入形状和步长，自动计算合适的padding_shape，以保持卷积过后，形状不变\n",
    "    \n",
    "    返回list[int]，[宽左，宽右，高上，高下]，为零填充的像素数\n",
    "    '''\n",
    "    padding=[]\n",
    "    height, width = input_shape\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_h, kernel_w = kernel_size, kernel_size\n",
    "    else:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "    if isinstance(stride, int):\n",
    "        stride_h, stride_w = stride, stride\n",
    "    else:\n",
    "        stride_h, stride_w = stride\n",
    "    \n",
    "    # 宽，(width - kernel_w + padding_w_left + padding_w_right) / stride_w + 1 == width\n",
    "    padding.append(((width-1)*stride_w - width + kernel_w) / 2)             # 整除，取下整\n",
    "    if ((width-1)*stride_w - width + kernel_w) % 2 == 0:\n",
    "        padding.append(((width-1)*stride_w - width + kernel_w) / 2)\n",
    "    else:\n",
    "        padding.append(((width-1)*stride_w - width + kernel_w) / 2 + 1)\n",
    "    # 高，也是类似\n",
    "    padding.append(((height-1)*stride_h - height + kernel_h) / 2)\n",
    "    if ((height-1)*stride_h - height + kernel_h) % 2 == 0:\n",
    "        padding.append(((height-1)*stride_h - height + kernel_h) / 2)\n",
    "    else:\n",
    "        padding.append(((height-1)*stride_h - height + kernel_h) / 2 + 1)\n",
    "    \n",
    "    return padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络中重复结构（卷积层）的构建函数，启发于VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(in_channels, out_channels, kernel_size, stride, padding, batch_normalize=1, activation='leaky'):\n",
    "    '''\n",
    "    构建yolov1中的卷积层, 形如pad->conv2d->batch_norm->activation function\n",
    "    \n",
    "    Parameters:\n",
    "    - in_channels, out_channels, kernel_size, stride, padding: 同nn.Conv2d\n",
    "    - batch_normalize: 是否要做批量归一化, 默认要做\n",
    "    - activation: 激活函数种类, 默认LeakyReLU\n",
    "    '''\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding))\n",
    "    if batch_normalize:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    if activation=='leaky':\n",
    "        layers.append(nn.LeakyReLU(inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构如图，图源自原论文，网络输入形如(batch_size, channel, height, width)\n",
    "\n",
    "<img src='./figs/yolov1_model.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中7x7代表将图片均匀划分成7x7个grid cell，每个grid cell负责判断是否有物体的中心点落在上面：\n",
    "\n",
    "<img src='./figs/yolov1_grid.png' style='zoom:50%'>\n",
    "\n",
    "每个grid对应一个长度为（5xB + class num）的向量：\n",
    "\n",
    "- B为每个grid负责回归的边界框个数；\n",
    "- 每个grid只能取B个中最好的一个作为预测值，即网络最多只能预测SxS个物体，论文中S=7，这也是YOLO-V1对小物体、密集物体效果不好的原因；\n",
    "- 5：每个边界框$(x_c, y_c, w, h, c)$，中心点位置、高宽、置信度（该box负责预测物体）；\n",
    "- class num：后面跟的ont hot向量，表示在该box负责预测物体的条件下，预测为类别i的条件概率（有疑问）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_v1(nn.Module):\n",
    "    '''\n",
    "    Yolo-v1的pytorch实现\n",
    "    \n",
    "    输入形状为448x448, B为每个grid预测的个数, Class_Num为分类数\n",
    "    '''\n",
    "    def __init__(self, B=2, Classes_Num=20):\n",
    "        super().__init__()\n",
    "        self.B = B\n",
    "        self.Classes_Num = Classes_Num\n",
    "        # 提前padding，保持各个块的卷积过后，形状不变；\n",
    "        # in:3x448x448, out:64x112x112\n",
    "        self.blk1 = nn.Sequential(\n",
    "            *create_conv_layer(3, 64, kernel_size=7, stride=2, padding=3),  # stride=2的conv和pool都是减高宽的\n",
    "            # nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=2),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:64x112x112, out:192x56x56\n",
    "        self.blk2 = nn.Sequential(\n",
    "            *create_conv_layer(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(192),\n",
    "            # nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:192x56x56, out:512x28x28\n",
    "        self.blk3 = nn.Sequential(\n",
    "            *create_conv_layer(192, 128, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(256, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:512x28x28, out:1024x14x14\n",
    "        self.blk4 = nn.Sequential(\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:1024x14x14, out:1024x14x14\n",
    "        self.blk4 = nn.Sequential(\n",
    "            *create_conv_layer(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        # in:1024x14x14, out:1024x7x7\n",
    "        self.blk5 = nn.Sequential(\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        # in:7x7x1024, out:7x7x30\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 1024, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 7 * 7 * (B*5 + Classes_Num)),\n",
    "            nn.Sigmoid()        # 增加sigmoid函数是为了将输出全部映射到(0,1)之间，因为如果出现负数或太大的数，后续计算loss会很麻烦\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 类GoogLeNet\n",
    "        # 也可换成resnet50，删除stage5后的全局池化和全连接层，增加一个带有空洞卷积的网络block（模仿detnet网络，增加感受野而不改变输出特征图大小）\n",
    "        x = self.blk1(x)\n",
    "        x = self.blk2(x)\n",
    "        x = self.blk3(x)\n",
    "        x = self.blk4(x)\n",
    "        # 后四层卷积\n",
    "        x = self.blk5(x)\n",
    "        # 为方便后处理，需要将输出构建成7x7x30，而不是30x7x7；为了符合直觉，需要做重排列将特征放在最后一维\n",
    "        # (batch_size, channel, height, width) -> (batch_size, height, width, channel)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # 展平+全连接层\n",
    "        x = self.fc(x)\n",
    "        # 重构\n",
    "        x = x.view((-1, 7, 7, (self.B*5 + self.Classes_Num)))\n",
    "\n",
    "    def initialize_params(self):\n",
    "        '''网络参数初始化'''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑问：(x,y,w,h,c)和p是否需要做softmax？？？\n",
    "\n",
    "需要，我们用的结果是归一化的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function:\n",
    "\n",
    "<img src='./figs/yolov1_loss.png' style='zoom:60%;' />\n",
    "\n",
    "- 掩码：\n",
    "  - $\\mathbb{1}_{i}^{obj}$：有gt bbox的中心点落在grid $i$中；\n",
    "  - $\\mathbb{1}_{ij}^{obj}$：有gt bbox的中心点落在grid $i$中，且该grid回归的的B个bbox中，第$j$个bbox与该gt bbox的IoU最大；\n",
    "  - $\\mathbb{1}_{ij}^{noobj} == not \\ (\\mathbb{1}_{ij}^{obj})$；\n",
    "- 第一项为中心点定位误差；\n",
    "- 第二项为宽高定位误差：\n",
    "  - 求根号可使小的bbox对误差更敏感；\n",
    "- 第三、四项为confidence回归误差：\n",
    "  - $\\mathbb{1}_{ij}^{obj}==1$时（第三项），$\\hat{C}_i$为该grid回归的第j个bbox，与中心点落在该grid的gt bbox的IoU；\n",
    "  - $\\mathbb{1}_{ij}^{obj}==0$时（第四项），$\\hat{C}_i$为0；\n",
    "  - $C_i$为预测值；\n",
    "- 第五项为类别预测误差；\n",
    "- 权重：\n",
    "  - 物体检测问题，是一个典型的类别数目不均衡的问题。其中49个格点，含有物体的格点往往只有3、4个，其余全是不含有物体的格点。此时如果不采取点措施，那么物体检测的mAP不会太高，因为模型更倾向于不含有物体的格点。\n",
    "  - $\\lambda_{\\mathbb{coord}}$：使网络更重视$(x_c,y_c,w,h)$的预测；\n",
    "  - $\\lambda_{noobj}$：对不负责预测obj的confidence loss，赋予小的权重；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_v1_loss(nn.Module):\n",
    "    '''\n",
    "    Yolo-v1的损失函数计算的pytorch实现\n",
    "\n",
    "    Parameters:\n",
    "    - img_shape: _size_2_t, [h, w]\n",
    "    - S: _size_2_t, 表示一张图片被划分成了[高x宽]个grid, 默认为7表示7x7\n",
    "    - B: int, 一个grid回归多少个bbox\n",
    "    - Class_num: int, 类别个数\n",
    "    - l_coord、l_noobj: 对应两个lambda\n",
    "    '''\n",
    "    def __init__(self, img_shape=448, S=7, B=2, Class_num=20, l_coord=5, l_noobj=0.5):\n",
    "        super().__init__()\n",
    "        # 图片的形状\n",
    "        if isinstance(img_shape, int):\n",
    "            self.img_h = img_shape\n",
    "            self.img_w = img_shape\n",
    "        elif hasattr(img_shape, '__iter__'):\n",
    "            self.img_h = img_shape[0]\n",
    "            self.img_w = img_shape[1]\n",
    "        # 网格的个数形状\n",
    "        if isinstance(S, int):\n",
    "            self.S_h = S\n",
    "            self.S_w = S\n",
    "        elif hasattr(S, '__iter__'):\n",
    "            self.S_h = S[0]\n",
    "            self.S_w = S[1]\n",
    "        self.grid_h = self.img_h / self.S_h     # 网格的高度和宽度\n",
    "        self.grid_w = self.img_w / self.S_w\n",
    "        self.B = B\n",
    "        self.Class_num = Class_num\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        '''\n",
    "        前向计算\n",
    "        - pred:  (batch_size, S_h, S_w, (B)*5+Class_num), 未经过预处理, 仍然只是Yolo_v1输出的sigmoid\n",
    "        - gt:    (batch_size, S_h, S_w, 5+Class_num), 已经经过预处理, 框成center形式(但中心点位置是相对于左上角的), 其中confidence代表1_i^{obj}, 类别成one-hot\n",
    "        '''\n",
    "        # 定义三个计算损失的变量 正样本定位损失 样本置信度损失 样本类别损失\n",
    "        loss_coord = 0\n",
    "        loss_confidence = 0\n",
    "        loss_classes = 0\n",
    "        iou_sum = 0\n",
    "        object_num = 0\n",
    "\n",
    "        mseLoss = nn.MSELoss()\n",
    "        # pred的预处理\n",
    "        for i in range(self.B):\n",
    "            pred[:, :, :, i*5+0] = pred[:, :, :, i*5+0] * self.grid_w   # x_c相对于grid左上角的位置，最大不能超过一个grid\n",
    "            pred[:, :, :, i*5+1] = pred[:, :, :, i*5+1] * self.grid_h   # y_c相对于grid左上角的位置\n",
    "            pred[:, :, :, i*5+2] = pred[:, :, :, i*5+2] * self.img_w    # w，最大不能超过整张图片\n",
    "            pred[:, :, :, i*5+3] = pred[:, :, :, i*5+3] * self.img_h    # h\n",
    "        \n",
    "        # loss计算\n",
    "        # 其实也可以使用矩阵运算优化，但保险起见，还是用了循环\n",
    "        for batch in range(len(pred)):\n",
    "            for row in range(self.S_h):\n",
    "                for col in range(self.S_w):\n",
    "                    pred_ = pred[batch, row, col, :]\n",
    "                    gt_ = gt[batch, row, col, :]\n",
    "                    if gt_[4] == 0:\n",
    "                        # [row, col]处的grid，没有gt框中心落在这，故B个框都是负样本，只需要加置信度loss\n",
    "                        for i in range(self.B):\n",
    "                            loss_confidence += self.l_noobj * torch.pow(pred_[i*5 + 4], 2)\n",
    "                    else:\n",
    "                        # [row, col]处的grid，有gt框中心落在这\n",
    "                        object_num += 1\n",
    "                        # 分别计算回归的B个框与gt的IoU\n",
    "                        iou = torch.zeros(self.B)\n",
    "                        gt_bbox = utils.bbox_center_to_corner(gt_[:4])\n",
    "                        for i in range(self.B):\n",
    "                            pred_bbox = utils.bbox_center_to_corner(pred_[i*5:(i*5+4)])\n",
    "                            iou[i] = utils.IoU(pred_bbox, gt_bbox)\n",
    "                        # 取IoU最大的成为正样本，其它B-1个成为负样本\n",
    "                        val, idx = iou.topk(1)\n",
    "                        # 定位loss\n",
    "                        loss_coord += self.l_coord * (torch.pow(pred_[idx*5] - gt_[0], 2) + torch.pow(pred_[idx*5+1] - gt_[1], 2)) + \\\n",
    "                                      self.l_coord * (torch.pow(torch.sqrt(pred_[idx*5+2]) - torch.sqrt(gt_[2]), 2) + \\\n",
    "                                                      torch.pow(torch.sqrt(pred_[idx*5+3]) - torch.sqrt(gt_[3]), 2))\n",
    "                        # 置信度loss\n",
    "                        iou_sum += val\n",
    "                        for i in range(self.B):\n",
    "                            if i == idx:\n",
    "                                # 正样本\n",
    "                                loss_confidence += torch.pow(pred_[i*5 + 4] - val, 2)\n",
    "                            else:\n",
    "                                # 负样本\n",
    "                                loss_confidence += self.l_noobj * torch.pow(pred_[i*5 + 4], 2)\n",
    "                        # 类别预测loss\n",
    "                        gt_class = gt_[5:]\n",
    "                        pred_class = pred_[(self.B*5):]\n",
    "                        loss_classes += mseLoss(gt_class, pred_class)\n",
    "        loss_coord /= len(pred)\n",
    "        loss_confidence /= len(pred)\n",
    "        loss_classes /= len(pred)\n",
    "        loss = loss_coord + loss_confidence + loss_classes\n",
    "        return loss, loss_coord, loss_confidence, loss_classes, iou_sum, object_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、预测阶段后处理：非极大值抑制（NMS）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3, 3, 3, 2, 2, 2, 1, 3, 1, 2, 2, 3, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 3,\n",
       "           3, 1, 1, 3, 3, 3, 3],\n",
       "          [1, 1, 1, 2, 3, 2, 2, 2, 2, 3, 3, 1, 3, 1, 1, 3, 2, 1, 3, 1, 3, 2, 1,\n",
       "           1, 2, 3, 1, 1, 2, 1]],\n",
       "\n",
       "         [[3, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 3, 1, 2, 1,\n",
       "           3, 2, 1, 3, 2, 1, 2],\n",
       "          [1, 1, 3, 3, 2, 2, 3, 3, 2, 1, 2, 1, 3, 2, 3, 3, 2, 3, 1, 2, 2, 1, 2,\n",
       "           1, 3, 1, 1, 2, 2, 3]]],\n",
       "\n",
       "\n",
       "        [[[2, 2, 2, 1, 2, 3, 1, 1, 2, 1, 1, 3, 2, 3, 2, 2, 2, 1, 3, 1, 1, 1, 2,\n",
       "           1, 3, 2, 1, 1, 1, 1],\n",
       "          [3, 3, 3, 3, 1, 1, 2, 1, 2, 2, 2, 3, 3, 2, 2, 1, 2, 2, 1, 3, 3, 2, 1,\n",
       "           3, 1, 2, 2, 3, 1, 1]],\n",
       "\n",
       "         [[2, 2, 3, 2, 2, 1, 1, 2, 3, 3, 2, 2, 2, 2, 1, 3, 1, 1, 2, 2, 2, 2, 2,\n",
       "           1, 3, 1, 2, 3, 3, 1],\n",
       "          [2, 3, 2, 3, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 1, 2, 1, 1, 2, 2, 1, 3, 1,\n",
       "           3, 1, 2, 1, 2, 1, 3]]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(1, 4, (2, 2, 2, 30))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3, 3, 2],\n",
       "          [1, 1, 2]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 3, 3]]],\n",
       "\n",
       "\n",
       "        [[[2, 2, 1],\n",
       "          [3, 3, 3]],\n",
       "\n",
       "         [[2, 3, 2],\n",
       "          [3, 2, 3]]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = torch.split(a, [1, 3, 26], dim=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 3]),\n",
       " tensor([3, 3, 2, 1, 1, 2, 1, 1, 1, 1, 3, 3, 2, 2, 1, 3, 3, 3, 2, 3, 2, 3, 2, 3]),\n",
       " tensor([[[[3, 3, 2],\n",
       "           [1, 1, 2]],\n",
       " \n",
       "          [[1, 1, 1],\n",
       "           [1, 3, 3]]],\n",
       " \n",
       " \n",
       "         [[[2, 2, 1],\n",
       "           [3, 3, 3]],\n",
       " \n",
       "          [[2, 3, 2],\n",
       "           [3, 2, 3]]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, y.flatten(), y.flatten(0, 2).reshape(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 2, 3],\n",
      "          [1, 3, 1]],\n",
      "\n",
      "         [[2, 1, 1],\n",
      "          [3, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 2, 2],\n",
      "          [3, 1, 1]],\n",
      "\n",
      "         [[3, 3, 2],\n",
      "          [2, 1, 1]]]])\n",
      "tensor([[0, 0, 0],\n",
      "        [1, 3, 1],\n",
      "        [2, 1, 1],\n",
      "        [3, 1, 2],\n",
      "        [3, 2, 2],\n",
      "        [3, 1, 1],\n",
      "        [3, 3, 2],\n",
      "        [2, 1, 1]])\n",
      "tensor([[[[1, 2, 3],\n",
      "          [1, 3, 1]],\n",
      "\n",
      "         [[2, 1, 1],\n",
      "          [3, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 2, 2],\n",
      "          [3, 1, 1]],\n",
      "\n",
      "         [[3, 3, 2],\n",
      "          [2, 1, 1]]]])\n",
      "tensor([[0, 0, 0],\n",
      "        [1, 3, 1],\n",
      "        [2, 1, 1],\n",
      "        [3, 1, 2],\n",
      "        [3, 2, 2],\n",
      "        [3, 1, 1],\n",
      "        [3, 3, 2],\n",
      "        [2, 1, 1]])\n",
      "tensor([[[[0, 0, 0],\n",
      "          [1, 3, 1]],\n",
      "\n",
      "         [[2, 1, 1],\n",
      "          [3, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 2, 2],\n",
      "          [3, 1, 1]],\n",
      "\n",
      "         [[3, 3, 2],\n",
      "          [2, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "b = y.flatten(0, -2)\n",
    "c = copy.deepcopy(y.flatten(0, -2))\n",
    "print(y)\n",
    "c[0] = 0\n",
    "print(c)\n",
    "print(y)\n",
    "b[0] = 0\n",
    "print(b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试torch.argsort：返回shape相同的tensor，选定的dim上，index从低到高为排好序的indices。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 1, 1, 1],\n",
       "         [0, 3, 1, 2, 1],\n",
       "         [0, 2, 3, 1, 1],\n",
       "         [3, 0, 0, 1, 2]],\n",
       "\n",
       "        [[3, 2, 3, 1, 2],\n",
       "         [3, 0, 3, 1, 2],\n",
       "         [2, 0, 2, 2, 0],\n",
       "         [0, 3, 1, 3, 2]],\n",
       "\n",
       "        [[1, 1, 1, 3, 0],\n",
       "         [0, 3, 0, 1, 2],\n",
       "         [2, 1, 3, 0, 0],\n",
       "         [0, 2, 1, 1, 1]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 4, (3, 4, 5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 1, 2, 1, 3],\n",
       "         [0, 2, 0, 0, 0],\n",
       "         [1, 0, 1, 2, 1],\n",
       "         [2, 3, 3, 3, 2]],\n",
       "\n",
       "        [[0, 3, 0, 3, 0],\n",
       "         [1, 0, 1, 2, 1],\n",
       "         [2, 1, 2, 0, 3],\n",
       "         [3, 2, 3, 1, 2]],\n",
       "\n",
       "        [[2, 1, 2, 0, 1],\n",
       "         [0, 3, 0, 1, 3],\n",
       "         [1, 0, 3, 3, 0],\n",
       "         [3, 2, 1, 2, 2]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.argsort(a, dim=1, descending=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不应用于训练阶段，只应用于训练完成后，应用模型进行预测的阶段。\n",
    "\n",
    "非极大值抑制（Non-Maximum Suppression）：\n",
    "- 首先，对每个grid cell负责的B个框，该box负责预测物体的置信度 * 该box负责预测物体的条件下是各个类别的条件概率\n",
    "  - 获得形如$batch\\_size * (S_h * S_w * B) * ClassNum$的全概率，和形如$batch\\_size * (S_h * S_w * B) * 4$的center格式的2D框；\n",
    "- 其次，将小于阈值的全概率置零；\n",
    "- 然后，对全概率，在dim=1上做argsort，获得降序的indices；\n",
    "- 最后，对每个类别，对该类别的每个mini batch，对该类别该mini batch的全概率降序的box1，对该类别该mini batch全概率小于该box1的每个box2，若box1和box2的IoU大于阈值，则将box2的全概率置零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yolo_v1_NMS(YOLO_pred, img_shape=448, S=7, B=2, Class_Num=20, threshold_confidence=0.5, threshold_IOU=0.5):\n",
    "    '''\n",
    "    非极大值抑制\n",
    "    - YOLO_pred: (batch_size, S_h, S_w, B*5+Class_Num)形状的张量\n",
    "    - threshold_confidence: 抹去低置信度的阈值\n",
    "    - threshold_IOU: 抹去高交并比的阈值\n",
    "    '''\n",
    "    # 参数计算\n",
    "    if isinstance(img_shape, int):\n",
    "        img_h = img_shape\n",
    "        img_w = img_shape\n",
    "    elif hasattr(img_shape, '__iter__'):\n",
    "        img_h = img_shape[0]\n",
    "        img_w = img_shape[1]\n",
    "    ## 网格的个数形状\n",
    "    if isinstance(S, int):\n",
    "        S_h = S\n",
    "        S_w = S\n",
    "    elif hasattr(S, '__iter__'):\n",
    "        S_h = S[0]\n",
    "        S_w = S[1]\n",
    "    grid_h = img_h / S_h     # 网格的高度和宽度\n",
    "    grid_w = img_w / S_w\n",
    "    \n",
    "    # 在最后一维上拆分YOLO_pred\n",
    "    split_list = []\n",
    "    for _ in range(B):\n",
    "        split_list.append(5)\n",
    "    split_list.append(Class_Num)\n",
    "    sp = torch.split(YOLO_pred, split_list, dim=-1)\n",
    "    # 构建batch_size * (S_h * S_w * B) * (4+Class_Num)的2D框和全概率矩阵\n",
    "    import copy\n",
    "    mats = []\n",
    "    for i in range(B):\n",
    "        box = copy.deepcopy(sp[i].flatten(1, 2))           # 框\n",
    "        conf = copy.deepcopy(sp[-1].flatten(1, 2))         # 预测各个类别的条件概率\n",
    "        # 将2D框中心点，从相对grid cell左上角的位置的归一化，处理成图片中的位置\n",
    "        for j in range(S_w):\n",
    "            for k in range(S_h):\n",
    "                idx = k*S_h+j\n",
    "                box[:, idx, 0] = (box[:, idx, 0] + j) * grid_w      # x_c\n",
    "                box[:, idx, 1] = (box[:, idx, 1] + k) * grid_h      # y_c\n",
    "        box[:, :, 2] = box[:, :, 2] * img_w    # w，最大不能超过整张图片\n",
    "        box[:, :, 3] = box[:, :, 3] * img_h    # h\n",
    "        conf[:] = conf * box[:, :, -1].reshape(box.shape[0], box.shape[1], 1)\n",
    "        box_shape = box[:, :, :-1].shape\n",
    "        box = box[:, :, :-1].flatten(0, 1)\n",
    "        mats.append(torch.concat([utils.bbox_center_to_corner(box).view(box_shape), conf], dim=-1))\n",
    "    bboxes = torch.concat(mats, dim=1)\n",
    "    \n",
    "    # Non maximum supression\n",
    "    prob = bboxes[:, :, 4:]\n",
    "    prob[prob<threshold_confidence] = 0         # 置信度阈值筛选（prob不是copy来的，是复制了索引，所以也会改变bboxes中的值）\n",
    "    indices = torch.argsort(prob, dim=1, descending=True)       # 第1维做argsort，即（S_h * S_w * B）展平后对应的维度\n",
    "    for i in range(Class_Num):\n",
    "        # 按类别处理\n",
    "        for batch in range(prob.shape[0]):\n",
    "            # 每一个小batch\n",
    "            for j in range(prob.shape[1]):\n",
    "                # 该batch，按第i类全概率的从大到小排列的每一个box\n",
    "                idx1 = indices[batch, j, i]\n",
    "                box1 = bboxes[batch, idx1, :4]\n",
    "                prob1 = bboxes[batch, idx1, 4+i]\n",
    "                if prob1 == 0:\n",
    "                    break\n",
    "                for k in range(prob.shape[1]-j-1):\n",
    "                    # 位于第j个box后面的每个box\n",
    "                    idx2 = indices[batch, j+k+1, i]\n",
    "                    box2 = bboxes[batch, idx2, :4]\n",
    "                    prob2 = bboxes[batch, idx2, 4+i]\n",
    "                    if prob2 == 0:\n",
    "                        break\n",
    "                    # 计算两个box的iou，若大于阈值，则box2的全概率置零\n",
    "                    if utils.IoU(box1, box2) > threshold_IOU:\n",
    "                        bboxes[batch, idx2, 4+i] = 0\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.4312e-01, 7.5455e-02, 2.1029e-01, 3.3986e-01, 1.8739e-01,\n",
       "           6.4411e-01, 1.1981e-01, 7.0344e-01, 1.8876e-02, 8.9807e-01,\n",
       "           3.9582e-01, 9.2258e-01, 7.0580e-01, 9.0151e-01, 6.1323e-01,\n",
       "           3.7566e-01, 1.1633e-01, 2.9468e-01, 8.3549e-01, 1.7574e-02,\n",
       "           7.2735e-02, 6.7653e-01, 7.8632e-01, 1.5139e-01, 3.1864e-01,\n",
       "           8.7997e-03, 2.6141e-01, 9.6208e-02, 6.6914e-01, 2.7080e-01,\n",
       "           2.0245e-01, 9.4872e-01, 7.2216e-01, 4.9454e-01, 3.3823e-01],\n",
       "          [7.5161e-01, 4.6720e-01, 3.0321e-01, 1.9213e-01, 1.8759e-01,\n",
       "           1.5533e-01, 3.1978e-01, 7.9518e-01, 9.3586e-01, 6.7862e-01,\n",
       "           6.3063e-01, 7.3962e-01, 2.0204e-01, 8.4402e-01, 3.0588e-01,\n",
       "           7.5737e-02, 9.1034e-01, 4.3822e-01, 8.4623e-01, 5.9952e-01,\n",
       "           3.2011e-01, 7.3935e-01, 6.2582e-01, 1.3060e-01, 4.8322e-01,\n",
       "           2.8883e-01, 8.4881e-01, 3.0274e-01, 6.4206e-01, 9.1845e-01,\n",
       "           8.5243e-01, 8.1788e-01, 7.0648e-01, 4.4671e-01, 8.3990e-01]],\n",
       "\n",
       "         [[8.5375e-01, 4.8586e-03, 9.1917e-01, 4.0783e-01, 8.9167e-01,\n",
       "           5.8959e-02, 7.7486e-01, 7.2568e-01, 4.9669e-01, 1.9623e-01,\n",
       "           3.8164e-01, 9.3559e-01, 9.3434e-02, 5.2963e-01, 7.4189e-02,\n",
       "           4.0925e-01, 3.9954e-01, 5.0806e-01, 3.4398e-01, 2.8744e-01,\n",
       "           2.2526e-02, 2.1056e-01, 1.3109e-01, 4.3196e-01, 9.7989e-01,\n",
       "           9.8522e-01, 8.3765e-01, 3.8885e-01, 8.5812e-01, 5.0605e-01,\n",
       "           9.1948e-02, 5.2259e-01, 2.4914e-01, 1.1320e-01, 1.2183e-01],\n",
       "          [7.6688e-01, 1.4395e-01, 2.8753e-01, 7.0116e-01, 1.0453e-01,\n",
       "           4.9415e-01, 9.3519e-01, 1.2882e-02, 1.8817e-01, 4.1718e-01,\n",
       "           2.6203e-02, 6.7907e-01, 6.6462e-02, 7.2470e-01, 1.7231e-01,\n",
       "           7.9967e-01, 6.3745e-01, 2.9193e-01, 9.7901e-01, 8.3771e-01,\n",
       "           2.1470e-01, 1.8819e-01, 2.5478e-01, 2.0849e-01, 2.2182e-01,\n",
       "           9.7376e-01, 7.9503e-01, 1.7934e-01, 9.0515e-01, 8.2528e-01,\n",
       "           4.0735e-01, 8.8193e-02, 4.0801e-01, 4.7149e-01, 6.2334e-01]]],\n",
       "\n",
       "\n",
       "        [[[4.4579e-01, 5.6768e-01, 1.8435e-01, 1.1713e-01, 3.4076e-01,\n",
       "           5.5051e-01, 1.8779e-02, 5.7250e-01, 8.7371e-01, 3.9937e-01,\n",
       "           2.2097e-01, 1.7993e-01, 8.7114e-01, 6.6972e-02, 8.5369e-01,\n",
       "           7.5383e-01, 8.2149e-01, 5.5939e-01, 2.8960e-01, 1.7708e-01,\n",
       "           3.7753e-01, 8.7759e-01, 2.1550e-01, 6.5861e-01, 4.2082e-01,\n",
       "           4.0111e-01, 1.6048e-01, 3.7080e-01, 8.0290e-01, 7.3376e-01,\n",
       "           9.1241e-01, 2.7862e-01, 5.7933e-01, 5.3583e-01, 2.6233e-01],\n",
       "          [7.3607e-01, 7.8051e-01, 6.9994e-01, 5.7264e-01, 3.7197e-01,\n",
       "           3.8362e-01, 2.3177e-01, 9.5549e-01, 7.5307e-01, 5.3477e-01,\n",
       "           7.5599e-01, 2.7712e-01, 6.5363e-01, 3.3407e-01, 2.9981e-01,\n",
       "           1.5108e-01, 3.4956e-01, 1.9766e-01, 4.2784e-01, 1.9215e-01,\n",
       "           7.9660e-01, 6.3972e-01, 5.2509e-01, 7.7251e-01, 5.9397e-01,\n",
       "           3.8585e-01, 9.1277e-01, 2.1119e-01, 8.8201e-01, 9.7256e-01,\n",
       "           6.9368e-01, 1.7506e-01, 9.4034e-01, 2.2797e-01, 7.5035e-01]],\n",
       "\n",
       "         [[8.2852e-01, 4.9148e-01, 5.8897e-01, 4.7023e-01, 9.8945e-01,\n",
       "           7.5222e-01, 9.4627e-01, 5.3809e-01, 5.9991e-01, 6.4119e-01,\n",
       "           9.3872e-01, 3.1411e-01, 7.4694e-01, 4.6220e-01, 8.7203e-02,\n",
       "           2.6034e-01, 1.0718e-01, 5.4800e-01, 4.6549e-01, 6.6988e-01,\n",
       "           6.5923e-01, 4.5231e-01, 9.8214e-01, 1.8823e-02, 8.5813e-02,\n",
       "           9.9796e-01, 2.2390e-01, 9.5903e-01, 2.4751e-01, 2.4011e-01,\n",
       "           8.6519e-01, 6.2769e-01, 6.7852e-02, 8.6482e-01, 9.8330e-01],\n",
       "          [4.9067e-01, 7.6646e-04, 1.3591e-01, 5.6058e-01, 5.6265e-01,\n",
       "           7.7684e-02, 2.7935e-01, 1.2712e-02, 3.4119e-01, 8.1132e-02,\n",
       "           9.3101e-02, 9.6352e-01, 2.0841e-01, 4.1152e-01, 1.4033e-01,\n",
       "           9.2335e-01, 8.0386e-01, 9.5430e-01, 7.9361e-01, 4.9303e-02,\n",
       "           1.8846e-01, 7.8561e-01, 3.5061e-02, 8.7935e-02, 3.5061e-01,\n",
       "           8.7998e-02, 7.9557e-01, 4.3942e-02, 7.1743e-01, 6.4584e-01,\n",
       "           1.4314e-01, 4.0669e-01, 4.1952e-01, 4.9110e-01, 1.2326e-01]]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 2, 2, 35))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 24])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yolo_v1_NMS(a, S=2, B=3).shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88474ca0ac5bd289cfc2b7cff5e070fc2eb64be0b001ce0011ec337ea9a55b21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
