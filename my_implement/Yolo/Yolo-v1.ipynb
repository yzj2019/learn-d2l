{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo-v1的pytorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依照yolov1.cfg、yolov1原论文、[github上一份pytorch实现](https://github.com/ProgrammerZhujinming/YOLOv1/blob/main/YOLO_V1_Model.py)；\n",
    "\n",
    "yolov1.cfg最后的fc层内容没太看懂；\n",
    "\n",
    "module.py是方便fine-tune的版本；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/yuzijian/miniconda3/envs/d2l/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试填充函数：\n",
    "- 输入的tensor：（batch，channel，height， width）\n",
    "- 填充pad：[宽左，宽右，高上，高下]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4, 4]),\n",
       " tensor([[[[ 0.6121,  0.9437, -0.5037, -0.8590],\n",
       "           [-0.2827,  1.1770,  0.0155, -0.1981],\n",
       "           [ 1.6830, -0.6217, -0.2851, -0.6225],\n",
       "           [-1.4199,  0.6816, -1.1158,  0.8052]]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 1, 4, 4))\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11, 7]),\n",
       " tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.6121,  0.9437, -0.5037, -0.8590,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.2827,  1.1770,  0.0155, -0.1981,  0.0000,  0.0000],\n",
       "           [ 0.0000,  1.6830, -0.6217, -0.2851, -0.6225,  0.0000,  0.0000],\n",
       "           [ 0.0000, -1.4199,  0.6816, -1.1158,  0.8052,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = F.pad(x, pad=[1,2,3,4])\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11, 7]),\n",
       " tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.6121,  0.9437, -0.5037, -0.8590,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.2827,  1.1770,  0.0155, -0.1981,  0.0000,  0.0000],\n",
       "           [ 0.0000,  1.6830, -0.6217, -0.2851, -0.6225,  0.0000,  0.0000],\n",
       "           [ 0.0000, -1.4199,  0.6816, -1.1158,  0.8052,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = nn.ZeroPad2d([1,2,3,4])\n",
    "y = f(x)\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = y[0, 0, 0, :4]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这个自适应padding形状的函数没用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_padding_shape(input_shape, kernel_size, stride):\n",
    "    '''\n",
    "    根据输入形状和步长，自动计算合适的padding_shape，以保持卷积过后，形状不变\n",
    "    \n",
    "    返回list[int]，[宽左，宽右，高上，高下]，为零填充的像素数\n",
    "    '''\n",
    "    padding=[]\n",
    "    height, width = input_shape\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_h, kernel_w = kernel_size, kernel_size\n",
    "    else:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "    if isinstance(stride, int):\n",
    "        stride_h, stride_w = stride, stride\n",
    "    else:\n",
    "        stride_h, stride_w = stride\n",
    "    \n",
    "    # 宽，(width - kernel_w + padding_w_left + padding_w_right) / stride_w + 1 == width\n",
    "    padding.append(((width-1)*stride_w - width + kernel_w) / 2)             # 整除，取下整\n",
    "    if ((width-1)*stride_w - width + kernel_w) % 2 == 0:\n",
    "        padding.append(((width-1)*stride_w - width + kernel_w) / 2)\n",
    "    else:\n",
    "        padding.append(((width-1)*stride_w - width + kernel_w) / 2 + 1)\n",
    "    # 高，也是类似\n",
    "    padding.append(((height-1)*stride_h - height + kernel_h) / 2)\n",
    "    if ((height-1)*stride_h - height + kernel_h) % 2 == 0:\n",
    "        padding.append(((height-1)*stride_h - height + kernel_h) / 2)\n",
    "    else:\n",
    "        padding.append(((height-1)*stride_h - height + kernel_h) / 2 + 1)\n",
    "    \n",
    "    return padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络中重复结构（卷积层）的构建函数，启发于VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(in_channels, out_channels, kernel_size, stride, padding, batch_normalize=1, activation='leaky'):\n",
    "    '''\n",
    "    构建yolov1中的卷积层, 形如pad->conv2d->batch_norm->activation function\n",
    "    \n",
    "    Parameters:\n",
    "    - in_channels, out_channels, kernel_size, stride, padding: 同nn.Conv2d\n",
    "    - batch_normalize: 是否要做批量归一化, 默认要做\n",
    "    - activation: 激活函数种类, 默认LeakyReLU\n",
    "    '''\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding))\n",
    "    if batch_normalize:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    if activation=='leaky':\n",
    "        layers.append(nn.LeakyReLU(inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构如图，图源自原论文，网络输入形如(batch_size, channel, height, width)\n",
    "\n",
    "<img src='./figs/yolov1_model.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中7x7代表将图片均匀划分成7x7个grid cell，每个grid cell负责判断是否有物体的中心点落在上面：\n",
    "\n",
    "<img src='./figs/yolov1_grid.png' style='zoom:50%'>\n",
    "\n",
    "每个grid对应一个长度为（5xB + class num）的向量：\n",
    "\n",
    "- B为每个grid负责回归的边界框个数；\n",
    "- 每个grid只能取B个中最好的一个作为预测值，即网络最多只能预测SxS个物体，论文中S=7，这也是YOLO-V1对小物体、密集物体效果不好的原因；\n",
    "- 5：每个边界框$(x_c, y_c, w, h, c)$，中心点位置、高宽、置信度（该box负责预测物体）；\n",
    "- class num：后面跟的ont hot向量，表示在该box负责预测物体的条件下，预测为类别i的条件概率（有疑问）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_v1(nn.Module):\n",
    "    '''\n",
    "    Yolo-v1的pytorch实现\n",
    "    \n",
    "    输入形状为448x448, B为每个grid预测的个数, Class_Num为分类数\n",
    "    '''\n",
    "    def __init__(self, B=2, Classes_Num=20):\n",
    "        super().__init__()\n",
    "        self.B = B\n",
    "        self.Classes_Num = Classes_Num\n",
    "        # 提前padding，保持各个块的卷积过后，形状不变；\n",
    "        # in:3x448x448, out:64x112x112\n",
    "        self.blk1 = nn.Sequential(\n",
    "            *create_conv_layer(3, 64, kernel_size=7, stride=2, padding=3),  # stride=2的conv和pool都是减高宽的\n",
    "            # nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=2),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:64x112x112, out:192x56x56\n",
    "        self.blk2 = nn.Sequential(\n",
    "            *create_conv_layer(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(192),\n",
    "            # nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:192x56x56, out:512x28x28\n",
    "        self.blk3 = nn.Sequential(\n",
    "            *create_conv_layer(192, 128, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(256, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:512x28x28, out:1024x14x14\n",
    "        self.blk4 = nn.Sequential(\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(512, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # in:1024x14x14, out:1024x14x14\n",
    "        self.blk4 = nn.Sequential(\n",
    "            *create_conv_layer(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "            *create_conv_layer(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        # in:1024x14x14, out:1024x7x7\n",
    "        self.blk5 = nn.Sequential(\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            *create_conv_layer(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        # in:7x7x1024, out:7x7x30\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 1024, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 7 * 7 * (B*5 + Classes_Num)),\n",
    "            nn.Sigmoid()        # 增加sigmoid函数是为了将输出全部映射到(0,1)之间，因为如果出现负数或太大的数，后续计算loss会很麻烦\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 类GoogLeNet\n",
    "        # 也可换成resnet50，删除stage5后的全局池化和全连接层，增加一个带有空洞卷积的网络block（模仿detnet网络，增加感受野而不改变输出特征图大小）\n",
    "        x = self.blk1(x)\n",
    "        x = self.blk2(x)\n",
    "        x = self.blk3(x)\n",
    "        x = self.blk4(x)\n",
    "        # 后四层卷积\n",
    "        x = self.blk5(x)\n",
    "        # 为方便后处理，需要将输出构建成7x7x30，而不是30x7x7；为了符合直觉，需要做重排列将特征放在最后一维\n",
    "        # (batch_size, channel, height, width) -> (batch_size, height, width, channel)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # 展平+全连接层\n",
    "        x = self.fc(x)\n",
    "        # 重构\n",
    "        x = x.view((-1, 7, 7, (self.B*5 + self.Classes_Num)))\n",
    "\n",
    "    def initialize_params(self):\n",
    "        '''网络参数初始化'''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑问：(x,y,w,h,c)和p是否需要做softmax？？？\n",
    "- 需要，我们用的结果是归一化的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function:\n",
    "\n",
    "<img src='./figs/yolov1_loss.png' style='zoom:60%;' />\n",
    "\n",
    "- 掩码：\n",
    "  - $\\mathbb{1}_{i}^{obj}$：有gt bbox的中心点落在grid $i$中；\n",
    "  - $\\mathbb{1}_{ij}^{obj}$：有gt bbox的中心点落在grid $i$中，且该grid回归的的B个bbox中，第$j$个bbox与该gt bbox的IoU最大；\n",
    "  - $\\mathbb{1}_{ij}^{noobj} == not \\ (\\mathbb{1}_{ij}^{obj})$；\n",
    "- 第一项为中心点定位误差；\n",
    "- 第二项为宽高定位误差：\n",
    "  - 求根号可使小的bbox对误差更敏感；\n",
    "- 第三、四项为confidence回归误差：\n",
    "  - $\\mathbb{1}_{ij}^{obj}==1$时（第三项），$\\hat{C}_i$为该grid回归的第j个bbox，与中心点落在该grid的gt bbox的IoU；\n",
    "  - $\\mathbb{1}_{ij}^{obj}==0$时（第四项），$\\hat{C}_i$为0；\n",
    "  - $C_i$为预测值；\n",
    "- 第五项为类别预测误差；\n",
    "- 权重：\n",
    "  - 物体检测问题，是一个典型的类别数目不均衡的问题。其中49个格点，含有物体的格点往往只有3、4个，其余全是不含有物体的格点。此时如果不采取点措施，那么物体检测的mAP不会太高，因为模型更倾向于不含有物体的格点。\n",
    "  - $\\lambda_{\\mathbb{coord}}$：使网络更重视$(x_c,y_c,w,h)$的预测；\n",
    "  - $\\lambda_{noobj}$：对不负责预测obj的confidence loss，赋予小的权重；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_v1_loss(nn.Module):\n",
    "    '''\n",
    "    Yolo-v1的损失函数计算的pytorch实现\n",
    "\n",
    "    Parameters:\n",
    "    - img_shape: _size_2_t, [h, w]\n",
    "    - S: _size_2_t, 表示一张图片被划分成了[高x宽]个grid, 默认为7表示7x7\n",
    "    - B: int, 一个grid回归多少个bbox\n",
    "    - Class_num: int, 类别个数\n",
    "    - l_coord、l_noobj: 对应两个lambda\n",
    "    '''\n",
    "    def __init__(self, img_shape=448, S=7, B=2, Class_num=20, l_coord=5, l_noobj=0.5):\n",
    "        super().__init__()\n",
    "        # 图片的形状\n",
    "        if isinstance(img_shape, int):\n",
    "            self.img_h = img_shape\n",
    "            self.img_w = img_shape\n",
    "        elif hasattr(img_shape, '__iter__'):\n",
    "            self.img_h = img_shape[0]\n",
    "            self.img_w = img_shape[1]\n",
    "        # 网格的个数形状\n",
    "        if isinstance(S, int):\n",
    "            self.S_h = S\n",
    "            self.S_w = S\n",
    "        elif hasattr(S, '__iter__'):\n",
    "            self.S_h = S[0]\n",
    "            self.S_w = S[1]\n",
    "        self.grid_h = self.img_h / self.S_h     # 网格的高度和宽度\n",
    "        self.grid_w = self.img_w / self.S_w\n",
    "        self.B = B\n",
    "        self.Class_num = Class_num\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        '''\n",
    "        前向计算\n",
    "        - pred:  (batch_size, S_h, S_w, (B)*5+Class_num), 未经过预处理, 仍然只是Yolo_v1输出的sigmoid\n",
    "        - gt:    (batch_size, S_h, S_w, 5+Class_num), 已经经过预处理, 框成center形式(但中心点位置是相对于左上角的), 其中confidence代表1_i^{obj}, 类别成one-hot\n",
    "        '''\n",
    "        # 定义三个计算损失的变量 正样本定位损失 样本置信度损失 样本类别损失\n",
    "        loss_coord = 0\n",
    "        loss_confidence = 0\n",
    "        loss_classes = 0\n",
    "        iou_sum = 0\n",
    "        object_num = 0\n",
    "\n",
    "        mseLoss = nn.MSELoss()\n",
    "        # pred的预处理\n",
    "        for i in range(self.B):\n",
    "            pred[:, :, :, i*5+0] = pred[:, :, :, i*5+0] * self.grid_w   # x_c相对于grid左上角的位置，最大不能超过一个grid\n",
    "            pred[:, :, :, i*5+1] = pred[:, :, :, i*5+1] * self.grid_h   # y_c相对于grid左上角的位置\n",
    "            pred[:, :, :, i*5+2] = pred[:, :, :, i*5+2] * self.img_w    # w，最大不能超过整张图片\n",
    "            pred[:, :, :, i*5+3] = pred[:, :, :, i*5+3] * self.img_h    # h\n",
    "        \n",
    "        # loss计算\n",
    "        # 其实也可以使用矩阵运算优化，但保险起见，还是用了循环\n",
    "        for batch in range(len(pred)):\n",
    "            for row in range(self.S_h):\n",
    "                for col in range(self.S_w):\n",
    "                    pred_ = pred[batch, row, col, :]\n",
    "                    gt_ = gt[batch, row, col, :]\n",
    "                    if gt_[4] == 0:\n",
    "                        # [row, col]处的grid，没有gt框中心落在这，故B个框都是负样本，只需要加置信度loss\n",
    "                        for i in range(self.B):\n",
    "                            loss_confidence += self.l_noobj * torch.pow(pred_[i*5 + 4], 2)\n",
    "                    else:\n",
    "                        # [row, col]处的grid，有gt框中心落在这\n",
    "                        object_num += 1\n",
    "                        # 分别计算回归的B个框与gt的IoU\n",
    "                        iou = torch.zeros(self.B)\n",
    "                        gt_bbox = utils.bbox_center_to_corner(gt_[:4])\n",
    "                        for i in range(self.B):\n",
    "                            pred_bbox = utils.bbox_center_to_corner(pred_[i*5:(i*5+4)])\n",
    "                            iou[i] = utils.IoU(pred_bbox, gt_bbox)\n",
    "                        # 取IoU最大的成为正样本，其它B-1个成为负样本\n",
    "                        val, idx = iou.topk(1)\n",
    "                        # 定位loss\n",
    "                        loss_coord += self.l_coord * (torch.pow(pred_[idx*5] - gt_[0], 2) + torch.pow(pred_[idx*5+1] - gt_[1], 2)) + \\\n",
    "                                      self.l_coord * (torch.pow(torch.sqrt(pred_[idx*5+2]) - torch.sqrt(gt_[2]), 2) + \\\n",
    "                                                      torch.pow(torch.sqrt(pred_[idx*5+3]) - torch.sqrt(gt_[3]), 2))\n",
    "                        # 置信度loss\n",
    "                        iou_sum += val\n",
    "                        for i in range(self.B):\n",
    "                            if i == idx:\n",
    "                                # 正样本\n",
    "                                loss_confidence += torch.pow(pred_[i*5 + 4] - val, 2)\n",
    "                            else:\n",
    "                                # 负样本\n",
    "                                loss_confidence += self.l_noobj * torch.pow(pred_[i*5 + 4], 2)\n",
    "                        # 类别预测loss\n",
    "                        gt_class = gt_[5:]\n",
    "                        pred_class = pred_[(self.B*5):]\n",
    "                        loss_classes += mseLoss(gt_class, pred_class)\n",
    "        loss_coord /= len(pred)\n",
    "        loss_confidence /= len(pred)\n",
    "        loss_classes /= len(pred)\n",
    "        loss = loss_coord + loss_confidence + loss_classes\n",
    "        return loss, loss_coord, loss_confidence, loss_classes, iou_sum, object_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、预测阶段后处理：非极大值抑制（NMS）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 2, 1, 1, 3, 1, 1, 2, 3, 3, 1, 3, 1, 1, 1, 1, 2, 1, 3, 2, 1, 3,\n",
       "           1, 2, 1, 1, 1, 1, 3],\n",
       "          [1, 3, 3, 3, 1, 2, 1, 3, 2, 3, 2, 1, 2, 3, 2, 3, 3, 1, 2, 3, 2, 3, 2,\n",
       "           1, 3, 1, 2, 2, 1, 1]],\n",
       "\n",
       "         [[3, 1, 1, 3, 1, 1, 2, 3, 1, 1, 3, 3, 2, 3, 1, 1, 2, 2, 3, 3, 2, 3, 1,\n",
       "           2, 2, 3, 2, 3, 3, 3],\n",
       "          [1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 3, 2, 1, 3, 1, 2, 3, 2, 3, 1, 3,\n",
       "           1, 3, 2, 1, 3, 1, 3]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 1, 1, 3, 1, 2, 3, 2, 1, 3, 2, 3,\n",
       "           3, 1, 1, 3, 1, 3, 1],\n",
       "          [1, 3, 3, 3, 3, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 3,\n",
       "           1, 1, 1, 2, 3, 1, 3]],\n",
       "\n",
       "         [[2, 3, 1, 3, 3, 2, 2, 3, 3, 1, 3, 2, 2, 1, 2, 2, 1, 3, 3, 3, 3, 2, 1,\n",
       "           2, 2, 3, 3, 1, 2, 3],\n",
       "          [3, 3, 2, 2, 1, 1, 2, 1, 3, 2, 1, 3, 2, 3, 1, 1, 3, 1, 3, 2, 1, 1, 2,\n",
       "           1, 3, 2, 3, 2, 1, 3]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(1, 4, (2, 2, 2, 30))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 2, 1],\n",
       "          [3, 3, 3]],\n",
       "\n",
       "         [[1, 1, 3],\n",
       "          [2, 1, 2]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 2],\n",
       "          [3, 3, 3]],\n",
       "\n",
       "         [[3, 1, 3],\n",
       "          [3, 2, 2]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = torch.split(a, [1, 3, 26], dim=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 3]),\n",
       " tensor([1, 2, 1, 3, 3, 3, 1, 1, 3, 2, 1, 2, 1, 1, 2, 3, 3, 3, 3, 1, 3, 3, 2, 2]),\n",
       " tensor([[[[1, 2, 1],\n",
       "           [3, 3, 3]],\n",
       " \n",
       "          [[1, 1, 3],\n",
       "           [2, 1, 2]]],\n",
       " \n",
       " \n",
       "         [[[1, 1, 2],\n",
       "           [3, 3, 3]],\n",
       " \n",
       "          [[3, 1, 3],\n",
       "           [3, 2, 2]]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, y.flatten(), y.flatten(0, 2).reshape(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 2, 1],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[1, 1, 3],\n",
      "          [2, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 2],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[3, 1, 3],\n",
      "          [3, 2, 2]]]])\n",
      "tensor([[0, 0, 0],\n",
      "        [3, 3, 3],\n",
      "        [1, 1, 3],\n",
      "        [2, 1, 2],\n",
      "        [1, 1, 2],\n",
      "        [3, 3, 3],\n",
      "        [3, 1, 3],\n",
      "        [3, 2, 2]])\n",
      "tensor([[[[1, 2, 1],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[1, 1, 3],\n",
      "          [2, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 2],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[3, 1, 3],\n",
      "          [3, 2, 2]]]])\n",
      "tensor([[0, 0, 0],\n",
      "        [3, 3, 3],\n",
      "        [1, 1, 3],\n",
      "        [2, 1, 2],\n",
      "        [1, 1, 2],\n",
      "        [3, 3, 3],\n",
      "        [3, 1, 3],\n",
      "        [3, 2, 2]])\n",
      "tensor([[[[0, 0, 0],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[1, 1, 3],\n",
      "          [2, 1, 2]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 2],\n",
      "          [3, 3, 3]],\n",
      "\n",
      "         [[3, 1, 3],\n",
      "          [3, 2, 2]]]])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "b = y.flatten(0, -2)\n",
    "c = copy.deepcopy(y.flatten(0, -2))\n",
    "print(y)\n",
    "c[0] = 0\n",
    "print(c)\n",
    "print(y)\n",
    "b[0] = 0\n",
    "print(b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试torch.argsort：返回shape相同的tensor，选定的dim上，index从低到高为排好序的indices。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 3, 3, 2, 0],\n",
       "         [3, 0, 2, 2, 3],\n",
       "         [1, 3, 3, 0, 3],\n",
       "         [3, 3, 0, 3, 0]],\n",
       "\n",
       "        [[1, 0, 3, 3, 0],\n",
       "         [3, 0, 3, 1, 2],\n",
       "         [0, 1, 0, 0, 2],\n",
       "         [0, 0, 1, 0, 3]],\n",
       "\n",
       "        [[0, 0, 1, 2, 0],\n",
       "         [3, 2, 2, 3, 0],\n",
       "         [2, 2, 2, 2, 2],\n",
       "         [1, 3, 3, 2, 3]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 4, (3, 4, 5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 3, 1],\n",
       "         [3, 2, 2, 0, 2],\n",
       "         [2, 3, 1, 1, 0],\n",
       "         [0, 1, 3, 2, 3]],\n",
       "\n",
       "        [[1, 2, 0, 0, 3],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [2, 1, 3, 2, 2],\n",
       "         [3, 3, 2, 3, 0]],\n",
       "\n",
       "        [[1, 3, 3, 1, 3],\n",
       "         [2, 1, 1, 0, 2],\n",
       "         [3, 2, 2, 2, 0],\n",
       "         [0, 0, 0, 3, 1]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.argsort(a, dim=1, descending=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不应用于训练阶段，只应用于训练完成后，应用模型进行预测的阶段。\n",
    "\n",
    "非极大值抑制（Non-Maximum Suppression）：\n",
    "- 首先，对每个grid cell负责的B个框，该box负责预测物体的置信度 * 该box负责预测物体的条件下是各个类别的条件概率\n",
    "  - 获得形如$batch\\_size * (S_h * S_w * B) * ClassNum$的全概率，和形如$batch\\_size * (S_h * S_w * B) * 4$的center格式的2D框；\n",
    "- 其次，将小于阈值的全概率置零；\n",
    "- 然后，对全概率，在dim=1上做argsort，获得降序的indices；\n",
    "- 最后，对每个类别，对该类别的每个mini batch，对该类别该mini batch的全概率降序的box1，对该类别该mini batch全概率小于该box1的每个box2，若box1和box2的IoU大于阈值，则将box2的全概率置零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yolo_v1_NMS(YOLO_pred, img_shape=448, S=7, B=2, Class_Num=20, threshold_confidence=0.5, threshold_IOU=0.5):\n",
    "    '''\n",
    "    非极大值抑制\n",
    "    - YOLO_pred: (batch_size, S_h, S_w, B*5+Class_Num)形状的张量\n",
    "    - threshold_confidence: 抹去低置信度的阈值\n",
    "    - threshold_IOU: 抹去高交并比的阈值\n",
    "    '''\n",
    "    # 参数计算\n",
    "    if isinstance(img_shape, int):\n",
    "        img_h = img_shape\n",
    "        img_w = img_shape\n",
    "    elif hasattr(img_shape, '__iter__'):\n",
    "        img_h = img_shape[0]\n",
    "        img_w = img_shape[1]\n",
    "    ## 网格的个数形状\n",
    "    if isinstance(S, int):\n",
    "        S_h = S\n",
    "        S_w = S\n",
    "    elif hasattr(S, '__iter__'):\n",
    "        S_h = S[0]\n",
    "        S_w = S[1]\n",
    "    grid_h = img_h / S_h     # 网格的高度和宽度\n",
    "    grid_w = img_w / S_w\n",
    "    \n",
    "    # 在最后一维上拆分YOLO_pred\n",
    "    split_list = []\n",
    "    for _ in range(B):\n",
    "        split_list.append(5)\n",
    "    split_list.append(Class_Num)\n",
    "    sp = torch.split(YOLO_pred, split_list, dim=-1)\n",
    "    # 构建batch_size * (S_h * S_w * B) * (4+Class_Num)的2D框和全概率矩阵\n",
    "    import copy\n",
    "    mats = []\n",
    "    for i in range(B):\n",
    "        box = copy.deepcopy(sp[i].flatten(1, 2))           # 框\n",
    "        conf = copy.deepcopy(sp[-1].flatten(1, 2))         # 预测各个类别的条件概率\n",
    "        # 将2D框中心点，从相对grid cell左上角的位置的归一化，处理成图片中的位置\n",
    "        for j in range(S_w):\n",
    "            for k in range(S_h):\n",
    "                idx = k*S_h+j\n",
    "                box[:, idx, 0] = (box[:, idx, 0] + j) * grid_w      # x_c\n",
    "                box[:, idx, 1] = (box[:, idx, 1] + k) * grid_h      # y_c\n",
    "        box[:, :, 2] = box[:, :, 2] * img_w    # w，最大不能超过整张图片\n",
    "        box[:, :, 3] = box[:, :, 3] * img_h    # h\n",
    "        conf[:] = conf * box[:, :, -1].reshape(box.shape[0], box.shape[1], 1)\n",
    "        mats.append(torch.concat([utils.bbox_center_to_corner(box[:, :, :-1]), conf], dim=-1))\n",
    "    bboxes = torch.concat(mats, dim=1)\n",
    "    \n",
    "    # Non maximum supression\n",
    "    prob = bboxes[:, :, 4:]\n",
    "    prob[prob<threshold_confidence] = 0         # 置信度阈值筛选（prob不是copy来的，是复制了索引，所以也会改变bboxes中的值）\n",
    "    indices = torch.argsort(prob, dim=1, descending=True)       # 第1维做argsort，即（S_h * S_w * B）展平后对应的维度\n",
    "    for i in range(Class_Num):\n",
    "        # 按类别处理\n",
    "        for batch in range(prob.shape[0]):\n",
    "            # 每一个小batch\n",
    "            for j in range(prob.shape[1]):\n",
    "                # 该batch，按第i类全概率的从大到小排列的每一个box\n",
    "                idx1 = indices[batch, j, i]\n",
    "                box1 = bboxes[batch, idx1, :4]\n",
    "                prob1 = bboxes[batch, idx1, 4+i]\n",
    "                if prob1 == 0:\n",
    "                    break\n",
    "                for k in range(prob.shape[1]-j-1):\n",
    "                    # 位于第j个box后面的每个box\n",
    "                    idx2 = indices[batch, j+k+1, i]\n",
    "                    box2 = bboxes[batch, idx2, :4]\n",
    "                    prob2 = bboxes[batch, idx2, 4+i]\n",
    "                    if prob2 == 0:\n",
    "                        break\n",
    "                    # 计算两个box的iou，若大于阈值，则box2的全概率置零\n",
    "                    if utils.IoU(box1, box2) > threshold_IOU:\n",
    "                        bboxes[batch, idx2, 4+i] = 0\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2909, 0.0566, 0.8975, 0.8648, 0.4648, 0.9205, 0.0990, 0.4620,\n",
       "           0.6357, 0.8942, 0.0236, 0.6944, 0.6561, 0.8878, 0.9821, 0.6583,\n",
       "           0.8177, 0.3093, 0.1912, 0.6971, 0.4892, 0.8955, 0.4803, 0.0099,\n",
       "           0.2163, 0.3113, 0.1072, 0.3823, 0.5122, 0.0401, 0.1430, 0.5318,\n",
       "           0.3729, 0.2996, 0.6083],\n",
       "          [0.4449, 0.0971, 0.8036, 0.9549, 0.0437, 0.2797, 0.6409, 0.4622,\n",
       "           0.9144, 0.7013, 0.7148, 0.3133, 0.0124, 0.1259, 0.3945, 0.7720,\n",
       "           0.7258, 0.8219, 0.3268, 0.0867, 0.8560, 0.7144, 0.1042, 0.1580,\n",
       "           0.3601, 0.0874, 0.7386, 0.7104, 0.9042, 0.8691, 0.2626, 0.5077,\n",
       "           0.7314, 0.3285, 0.4286]],\n",
       "\n",
       "         [[0.2068, 0.1682, 0.0337, 0.6131, 0.3510, 0.2399, 0.1452, 0.2914,\n",
       "           0.3356, 0.9604, 0.2175, 0.8654, 0.3242, 0.4736, 0.9332, 0.2088,\n",
       "           0.6001, 0.2149, 0.2930, 0.2897, 0.4015, 0.1565, 0.6218, 0.0914,\n",
       "           0.8608, 0.3013, 0.6956, 0.8515, 0.2207, 0.9866, 0.9445, 0.1494,\n",
       "           0.0937, 0.7350, 0.8727],\n",
       "          [0.8732, 0.9126, 0.6104, 0.5630, 0.9860, 0.3873, 0.5204, 0.2843,\n",
       "           0.5341, 0.2013, 0.9382, 0.3897, 0.3720, 0.6366, 0.8400, 0.7169,\n",
       "           0.4726, 0.0733, 0.6649, 0.4934, 0.8538, 0.9181, 0.1124, 0.3547,\n",
       "           0.1811, 0.2881, 0.9547, 0.8919, 0.0115, 0.3103, 0.4821, 0.2259,\n",
       "           0.7590, 0.5223, 0.2074]]],\n",
       "\n",
       "\n",
       "        [[[0.5643, 0.8703, 0.0118, 0.8710, 0.0043, 0.0167, 0.4709, 0.0037,\n",
       "           0.6135, 0.6670, 0.4343, 0.3835, 0.0778, 0.6095, 0.2784, 0.7102,\n",
       "           0.9931, 0.3295, 0.4027, 0.5881, 0.4868, 0.6444, 0.1137, 0.6535,\n",
       "           0.6712, 0.2413, 0.3830, 0.7269, 0.0601, 0.8985, 0.3410, 0.6935,\n",
       "           0.3061, 0.6407, 0.6198],\n",
       "          [0.9588, 0.5939, 0.0854, 0.1434, 0.3058, 0.3496, 0.5662, 0.4984,\n",
       "           0.2352, 0.3292, 0.8193, 0.1735, 0.0695, 0.3670, 0.1880, 0.7745,\n",
       "           0.3895, 0.5896, 0.9552, 0.9991, 0.0076, 0.6650, 0.9559, 0.4163,\n",
       "           0.4124, 0.3499, 0.6713, 0.0775, 0.7873, 0.2899, 0.4368, 0.9098,\n",
       "           0.2483, 0.5199, 0.4045]],\n",
       "\n",
       "         [[0.5651, 0.9546, 0.2232, 0.8008, 0.4166, 0.0168, 0.2204, 0.8445,\n",
       "           0.8485, 0.5726, 0.4404, 0.0289, 0.0242, 0.1004, 0.8040, 0.1195,\n",
       "           0.3914, 0.7523, 0.9917, 0.0897, 0.7568, 0.0701, 0.6058, 0.9602,\n",
       "           0.8499, 0.6469, 0.6064, 0.8138, 0.9780, 0.8694, 0.9850, 0.6679,\n",
       "           0.3704, 0.9246, 0.9470],\n",
       "          [0.4133, 0.8591, 0.3675, 0.3737, 0.5949, 0.7629, 0.0798, 0.5637,\n",
       "           0.1963, 0.9310, 0.2173, 0.3210, 0.8266, 0.6346, 0.5709, 0.9390,\n",
       "           0.3118, 0.0506, 0.9790, 0.5465, 0.6280, 0.4873, 0.0332, 0.9339,\n",
       "           0.9606, 0.0052, 0.6394, 0.6276, 0.3962, 0.8300, 0.5314, 0.4159,\n",
       "           0.5684, 0.5674, 0.2139]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 2, 2, 35))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yolo_v1_NMS(a, S=2, B=3).shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88474ca0ac5bd289cfc2b7cff5e070fc2eb64be0b001ce0011ec337ea9a55b21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
