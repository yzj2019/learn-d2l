## 从全连接层到卷积

MLP，没有预先假设任何与特征交互相关的先验结构。

### 1.不变性

图片中的item，往往具有一定解构特性。一只猫在图片上半部和图片下半部，它都是一只猫，且一般不会填满整张图片。

- 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
- 局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测。

### 2.限制MLP

#### 2.1重写二维输入的MLP

二维输入的MLP，需要四阶权重张量W，和二阶偏置参数U，可以形式化地表示为：
$$
\begin{aligned}
\matrix{[H]}_{i,j} &= [U]_{i,j} + \sum_k\sum_l[W]_{i,j,k,l}[X]_{k,l} \\
&= [U]_{i,j} + \sum_a\sum_b[V]_{i,j,a,b}[X]_{i+a,j+b}
\end{aligned}
$$
V和W只是形式上的转换；索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。

#### 2.2平移不变性

引入平移不变性，意味着检测对象在输入X中的平移，应该仅仅导致隐藏表示H中的平移。

通过使U、V不依赖于i、j，可以做到这一点：
$$
\matrix{[H]}_{i,j} = u + \sum_a\sum_b[V]_{a,b}[X]_{i+a,j+b}
$$
u变为标量，V变为二阶。

#### 2.3局部性

引入局部性，意味着H不应该依赖于输入的X全部元素。

设置索引a和b偏移的阈值，可以做到这一点：
$$
\matrix{[H]}_{i,j} = u + \sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{a,b}[X]_{i+a,j+b}
$$
上式就是一个卷积层。$V$被称为卷积核（convolution kernel）或者滤波器（filter）。

“卷积”一词来源于数学中函数的卷积运算，函数$f$的索引$(a,b)$和$g$的索引$(i-a, j-b)$对应求和；虽然不是使用+而是使用差值，但这只是符号性的，只是“互相关”和“卷积”的区别。

### 3.扩展

#### 3.1输入的通道

图像一般包含RGB三种原色，三通道；故卷积核应相应地扩展为三阶：(通道维, 高, 宽)；

此时的卷积操作，应该是在2d图像平面卷积后，在通道维求和压缩：
$$
\matrix{[H]}_{i,j} = u + \sum_c\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{c,a,b}[X]_{i+a,j+b}
$$

#### 3.2输出的通道

希望卷积操作能够学习出多种特征，故需要输出也是多通道的；

卷积核相应地扩展为四阶：(输出通道维，输入通道维，高，宽)；

相应的卷积操作：
$$
\matrix{[H]}_{d,i,j} = u + \sum_c\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{d,c,a,b}[X]_{i+a,j+b}
$$

#### 3.3mini batch

e = batch_size
$$
\matrix{[H]}_{e,d,i,j} = u + \sum_c\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{e,d,c,a,b}[X]_{i+a,j+b}
$$
